{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, to_timestamp\n",
        "# List of stock symbols\n",
        "stocks = [\"MSFT\", \"AAPL\", \"GOOGL\", \"AMZN\"]\n",
        "\n",
        "# Read all CSV files into a list of DataFrames\n",
        "dfs = []\n",
        "for stock in stocks:\n",
        "    df = spark.read.csv(f\"abfss://bronze@stockmarketdatalake123.dfs.core.windows.net/stock_data_raw/{stock}_stock.csv\", header=True, inferSchema=True)\n",
        "    dfs.append(df)\n",
        "\n",
        "# Combine all DataFrames into one\n",
        "combined_df = dfs[0]\n",
        "for df in dfs[1:]:\n",
        "    combined_df = combined_df.union(df)\n",
        "\n",
        "# Clean the data\n",
        "# Remove null values\n",
        "df_clean = combined_df.na.drop()\n",
        "\n",
        "# Convert Date column to timestamp\n",
        "df_clean = df_clean.withColumn(\"Date\", to_timestamp(\"Date\", \"yyyy-MM-dd\"))\n",
        "\n",
        "# Filter for the desired date range (October 1, 2024, to March 21, 2025)\n",
        "df_clean = df_clean.filter((df_clean.Date >= \"2024-10-01\") & (df_clean.Date <= \"2025-03-21\"))\n",
        "\n",
        "# Save to the silver layer as Parquet\n",
        "df_clean.write.mode(\"overwrite\").parquet(\"abfss://silver@stockmarketdatalake123.dfs.core.windows.net/stock_data_clean/\")\n",
        "\n",
        "# Display the cleaned data\n",
        "df_clean.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Read the cleaned data from the silver layer\n",
        "df_clean = spark.read.parquet(\"abfss://silver@stockmarketdatalake123.dfs.core.windows.net/stock_data_clean/\")\n",
        "\n",
        "# Define a window for the 7-day moving average, partitioned by Symbol\n",
        "window_spec = Window.partitionBy(\"Symbol\").orderBy(\"Date\").rowsBetween(-6, 0)\n",
        "\n",
        "# Compute the 7-day moving average for the Close price\n",
        "df_gold = df_clean.withColumn(\"moving_avg_7d\", avg(\"Close\").over(window_spec))\n",
        "\n",
        "# Save to the gold layer as Parquet\n",
        "df_gold.write.mode(\"overwrite\").parquet(\"abfss://gold@stockmarketdatalake123.dfs.core.windows.net/stock_data_aggregated/\")\n",
        "\n",
        "# Display the result\n",
        "df_gold.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add this code to your notebook to check the silver layer data\n",
        "df_check = spark.read.parquet(\"abfss://silver@stockmarketdatalake123.dfs.core.windows.net/stock_data_clean/\")\n",
        "df_check.groupBy(\"Symbol\").count().show()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
